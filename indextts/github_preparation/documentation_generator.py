"""
Documentation Generator for IndexTTS2 GitHub Preparation

This module provides bilingual documentation generation capabilities for the IndexTTS2 project,
creating comprehensive README files and system documentation with proper navigation and
installation instructions using UV package manager.
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import yaml


class DocumentationGenerator:
    """
    Generates bilingual documentation for IndexTTS2 project.
    
    Creates comprehensive README files and system documentation with:
    - English first, Chinese second structure
    - Navigation links to Chinese sections
    - Current IndexTTS2 capabilities and features
    - UV package manager installation instructions
    """
    
    def __init__(self, project_root: str = "."):
        """
        Initialize the documentation generator.
        
        Args:
            project_root: Root directory of the project
        """
        self.project_root = Path(project_root)
        self.config_path = self.project_root / "checkpoints" / "config.yaml"
        self.pyproject_path = self.project_root / "pyproject.toml"
        
    def get_project_metadata(self) -> Dict:
        """Extract project metadata from pyproject.toml and config files."""
        metadata = {
            "name": "IndexTTS2",
            "version": "2.0.0",
            "description": "Advanced Text-to-Speech with Emotion Control and Duration Precision",
            "author": "IndexTeam",
            "python_requires": ">=3.10",
            "main_dependencies": [
                "torch>=2.8.0",
                "torchaudio",
                "transformers",
                "gradio>=5.44.0",
                "librosa",
                "jieba",
                "g2p-en"
            ]
        }
        
        # Try to read from pyproject.toml if it exists
        if self.pyproject_path.exists():
            try:
                import tomllib
                with open(self.pyproject_path, 'rb') as f:
                    pyproject_data = tomllib.load(f)
                    project_info = pyproject_data.get('project', {})
                    metadata.update({
                        "name": project_info.get('name', metadata['name']),
                        "version": project_info.get('version', metadata['version']),
                        "description": project_info.get('description', metadata['description']),
                        "python_requires": project_info.get('requires-python', metadata['python_requires'])
                    })
            except Exception:
                pass  # Use defaults if parsing fails
                
        return metadata
    
    def generate_navigation_links(self) -> str:
        """Generate navigation links to Chinese sections."""
        return """## ðŸŒ Language / è¯­è¨€

- [English Documentation](#english-documentation) 
- [ä¸­æ–‡æ–‡æ¡£](#ä¸­æ–‡æ–‡æ¡£)

---
"""

    def generate_english_content(self, metadata: Dict) -> str:
        """Generate English documentation content."""
        return f"""# English Documentation

# {metadata['name']} - Advanced Text-to-Speech System

{metadata['description']}

## ðŸš€ Key Features

- **Zero-shot Voice Cloning**: Generate speech in any voice using just a short audio prompt
- **Emotional Expression Control**: Independent control over timbre and emotion through multiple modalities:
  - Audio-based emotion reference
  - Vector-based emotion control (8 emotions: happy, angry, sad, afraid, disgusted, melancholic, surprised, calm)
  - Text-based emotion descriptions via fine-tuned Qwen model
- **Duration Control**: First autoregressive TTS model with precise synthesis duration control
- **Multilingual Support**: Chinese and English text synthesis
- **High-Quality Output**: 22kHz audio generation with BigVGAN vocoder

## ðŸ—ï¸ Architecture

- **GPT-based Autoregressive Model**: Core text-to-speech generation
- **Semantic Codec**: MaskGCT-based semantic representation
- **S2Mel Module**: Semantic-to-mel spectrogram conversion with diffusion
- **BigVGAN Vocoder**: High-quality mel-to-waveform conversion
- **Emotion Control System**: Qwen-based emotion understanding and vector mapping

## ðŸ“‹ Requirements

- Python {metadata['python_requires']}
- CUDA 12.8+ (for GPU acceleration)
- 8GB+ VRAM recommended
- UV package manager (required)

## ðŸ› ï¸ Installation

### 1. Install UV Package Manager

```bash
# Install UV package manager first
pip install -U uv
```

### 2. Clone Repository

```bash
git clone https://github.com/cs2764/index-tts2-ext.git
cd index-tts2-ext
```

### 3. Install Dependencies

```bash
# Install all dependencies (recommended)
uv sync --all-extras

# Or install specific features only
uv sync --extra webui --extra deepspeed
```

### 4. Download Models

#### Via HuggingFace (International)
```bash
uv tool install "huggingface_hub[cli]"
hf download IndexTeam/IndexTTS-2 --local-dir=checkpoints
```

#### Via ModelScope (China)
```bash
uv tool install "modelscope"
modelscope download --model IndexTeam/IndexTTS-2 --local_dir checkpoints
```

## ðŸš€ Quick Start

### Web Interface (Recommended)
```bash
uv run webui.py
```

### Command Line Interface
```bash
# Basic usage
uv run indextts/cli.py "Hello world" -v examples/voice_01.wav

# With emotion control
uv run indextts/cli.py "I'm so happy!" -v examples/voice_01.wav --emotion happy
```

### Python API
```python
from indextts.infer_v2 import IndexTTS2

# Initialize model
tts = IndexTTS2()

# Generate speech
audio = tts.infer(
    text="Hello, this is IndexTTS2!",
    prompt_audio="examples/voice_01.wav",
    emotion="happy"
)
```

## ðŸŽ¯ Use Cases

- **Voice Dubbing**: Precise timing control for video synchronization
- **Audiobook Narration**: Emotional storytelling with consistent voice
- **Multilingual Content**: Chinese and English content creation
- **Interactive Applications**: Expressive speech for chatbots and assistants

## âš¡ Performance Optimization

- **FP16 Inference**: Use `--fp16` flag for reduced VRAM usage
- **DeepSpeed**: Use `--deepspeed` for potential acceleration
- **CUDA Kernels**: Use `--cuda_kernel` for BigVGAN optimization
- **Device Auto-detection**: Supports CUDA, XPU, MPS, CPU

## ðŸ§ª Testing

```bash
# Check GPU capability
uv run tools/gpu_check.py

# Run regression tests
uv run tests/regression_test.py
```

## ðŸ“ Project Structure

```
â”œâ”€â”€ indextts/           # Main package source code
â”œâ”€â”€ checkpoints/        # Model weights and configuration
â”œâ”€â”€ examples/           # Sample audio files and test cases
â”œâ”€â”€ tests/              # Test scripts and sample data
â”œâ”€â”€ tools/              # Utility scripts and helpers
â”œâ”€â”€ webui.py           # Gradio web interface
â””â”€â”€ pyproject.toml     # Package configuration
```

## ðŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Built on PyTorch and HuggingFace Transformers
- Uses BigVGAN for high-quality vocoding
- Emotion understanding powered by Qwen model

---
"""

    def generate_chinese_content(self, metadata: Dict) -> str:
        """Generate Chinese documentation content."""
        return f"""# ä¸­æ–‡æ–‡æ¡£

# {metadata['name']} - å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿ

{metadata['description']}

## ðŸš€ æ ¸å¿ƒç‰¹æ€§

- **é›¶æ ·æœ¬è¯­éŸ³å…‹éš†**: ä»…ä½¿ç”¨çŸ­éŸ³é¢‘æç¤ºå³å¯ç”Ÿæˆä»»æ„å£°éŸ³çš„è¯­éŸ³
- **æƒ…æ„Ÿè¡¨è¾¾æŽ§åˆ¶**: é€šè¿‡å¤šç§æ¨¡æ€ç‹¬ç«‹æŽ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿ:
  - åŸºäºŽéŸ³é¢‘çš„æƒ…æ„Ÿå‚è€ƒ
  - åŸºäºŽå‘é‡çš„æƒ…æ„ŸæŽ§åˆ¶ï¼ˆ8ç§æƒ…æ„Ÿï¼šå¼€å¿ƒã€æ„¤æ€’ã€æ‚²ä¼¤ã€ææƒ§ã€åŽŒæ¶ã€å¿§éƒã€æƒŠè®¶ã€å¹³é™ï¼‰
  - é€šè¿‡å¾®è°ƒQwenæ¨¡åž‹è¿›è¡ŒåŸºäºŽæ–‡æœ¬çš„æƒ…æ„Ÿæè¿°
- **æ—¶é•¿æŽ§åˆ¶**: é¦–ä¸ªå…·æœ‰ç²¾ç¡®åˆæˆæ—¶é•¿æŽ§åˆ¶çš„è‡ªå›žå½’TTSæ¨¡åž‹
- **å¤šè¯­è¨€æ”¯æŒ**: ä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬åˆæˆ
- **é«˜è´¨é‡è¾“å‡º**: ä½¿ç”¨BigVGANå£°ç å™¨ç”Ÿæˆ22kHzéŸ³é¢‘

## ðŸ—ï¸ æž¶æž„ç»„ä»¶

- **åŸºäºŽGPTçš„è‡ªå›žå½’æ¨¡åž‹**: æ ¸å¿ƒæ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆ
- **è¯­ä¹‰ç¼–è§£ç å™¨**: åŸºäºŽMaskGCTçš„è¯­ä¹‰è¡¨ç¤º
- **S2Melæ¨¡å—**: ä½¿ç”¨æ‰©æ•£çš„è¯­ä¹‰åˆ°æ¢…å°”é¢‘è°±è½¬æ¢
- **BigVGANå£°ç å™¨**: é«˜è´¨é‡æ¢…å°”é¢‘è°±åˆ°æ³¢å½¢è½¬æ¢
- **æƒ…æ„ŸæŽ§åˆ¶ç³»ç»Ÿ**: åŸºäºŽQwençš„æƒ…æ„Ÿç†è§£å’Œå‘é‡æ˜ å°„

## ðŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python {metadata['python_requires']}
- CUDA 12.8+ï¼ˆç”¨äºŽGPUåŠ é€Ÿï¼‰
- æŽ¨è8GB+æ˜¾å­˜
- UVåŒ…ç®¡ç†å™¨ï¼ˆå¿…éœ€ï¼‰

## ðŸ› ï¸ å®‰è£…æŒ‡å—

### 1. å®‰è£…UVåŒ…ç®¡ç†å™¨

```bash
# é¦–å…ˆå®‰è£…UVåŒ…ç®¡ç†å™¨
pip install -U uv
```

### 2. å…‹éš†ä»“åº“

```bash
git clone https://github.com/cs2764/index-tts2-ext.git
cd index-tts2-ext
```

### 3. å®‰è£…ä¾èµ–

```bash
# å®‰è£…æ‰€æœ‰ä¾èµ–ï¼ˆæŽ¨èï¼‰
uv sync --all-extras

# æˆ–ä»…å®‰è£…ç‰¹å®šåŠŸèƒ½
uv sync --extra webui --extra deepspeed
```

### 4. ä¸‹è½½æ¨¡åž‹

#### é€šè¿‡HuggingFaceï¼ˆå›½é™…ï¼‰
```bash
uv tool install "huggingface_hub[cli]"
hf download IndexTeam/IndexTTS-2 --local-dir=checkpoints
```

#### é€šè¿‡ModelScopeï¼ˆä¸­å›½ï¼‰
```bash
uv tool install "modelscope"
modelscope download --model IndexTeam/IndexTTS-2 --local_dir checkpoints
```

## ðŸš€ å¿«é€Ÿå¼€å§‹

### Webç•Œé¢ï¼ˆæŽ¨èï¼‰
```bash
uv run webui.py
```

### å‘½ä»¤è¡Œç•Œé¢
```bash
# åŸºæœ¬ç”¨æ³•
uv run indextts/cli.py "ä½ å¥½ä¸–ç•Œ" -v examples/voice_01.wav

# å¸¦æƒ…æ„ŸæŽ§åˆ¶
uv run indextts/cli.py "æˆ‘å¾ˆå¼€å¿ƒï¼" -v examples/voice_01.wav --emotion happy
```

### Python API
```python
from indextts.infer_v2 import IndexTTS2

# åˆå§‹åŒ–æ¨¡åž‹
tts = IndexTTS2()

# ç”Ÿæˆè¯­éŸ³
audio = tts.infer(
    text="ä½ å¥½ï¼Œè¿™æ˜¯IndexTTS2ï¼",
    prompt_audio="examples/voice_01.wav",
    emotion="happy"
)
```

## ðŸŽ¯ åº”ç”¨åœºæ™¯

- **é…éŸ³åˆ¶ä½œ**: ç²¾ç¡®æ—¶é•¿æŽ§åˆ¶ç”¨äºŽè§†é¢‘åŒæ­¥
- **æœ‰å£°è¯»ç‰©**: æƒ…æ„Ÿä¸°å¯Œçš„å™è¿°ï¼Œå£°éŸ³ä¸€è‡´
- **å¤šè¯­è¨€å†…å®¹**: ä¸­è‹±æ–‡å†…å®¹åˆ›ä½œ
- **äº¤äº’åº”ç”¨**: ä¸ºèŠå¤©æœºå™¨äººå’ŒåŠ©æ‰‹æä¾›å¯Œæœ‰è¡¨çŽ°åŠ›çš„è¯­éŸ³

## âš¡ æ€§èƒ½ä¼˜åŒ–

- **FP16æŽ¨ç†**: ä½¿ç”¨`--fp16`æ ‡å¿—å‡å°‘æ˜¾å­˜ä½¿ç”¨
- **DeepSpeed**: ä½¿ç”¨`--deepspeed`è¿›è¡Œæ½œåœ¨åŠ é€Ÿ
- **CUDAå†…æ ¸**: ä½¿ç”¨`--cuda_kernel`ä¼˜åŒ–BigVGAN
- **è®¾å¤‡è‡ªåŠ¨æ£€æµ‹**: æ”¯æŒCUDAã€XPUã€MPSã€CPU

## ðŸ§ª æµ‹è¯•

```bash
# æ£€æŸ¥GPUèƒ½åŠ›
uv run tools/gpu_check.py

# è¿è¡Œå›žå½’æµ‹è¯•
uv run tests/regression_test.py
```

## ðŸ“ é¡¹ç›®ç»“æž„

```
â”œâ”€â”€ indextts/           # ä¸»è¦åŒ…æºä»£ç 
â”œâ”€â”€ checkpoints/        # æ¨¡åž‹æƒé‡å’Œé…ç½®
â”œâ”€â”€ examples/           # ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶å’Œæµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ tests/              # æµ‹è¯•è„šæœ¬å’Œç¤ºä¾‹æ•°æ®
â”œâ”€â”€ tools/              # å®žç”¨è„šæœ¬å’ŒåŠ©æ‰‹
â”œâ”€â”€ webui.py           # Gradioç½‘é¡µç•Œé¢
â””â”€â”€ pyproject.toml     # åŒ…é…ç½®
```

## ðŸ¤ è´¡çŒ®

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºæ‚¨çš„åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æŽ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. æ‰“å¼€Pull Request

## ðŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚

## ðŸ™ è‡´è°¢

- åŸºäºŽPyTorchå’ŒHuggingFace Transformersæž„å»º
- ä½¿ç”¨BigVGANè¿›è¡Œé«˜è´¨é‡å£°ç åŒ–
- æƒ…æ„Ÿç†è§£ç”±Qwenæ¨¡åž‹æä¾›æ”¯æŒ

---
"""

    def update_readme(self) -> None:
        """Generate and update the main README.md file."""
        metadata = self.get_project_metadata()
        
        # Generate complete README content
        readme_content = self.generate_navigation_links()
        readme_content += self.generate_english_content(metadata)
        readme_content += self.generate_chinese_content(metadata)
        
        # Write to README.md
        readme_path = self.project_root / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… Updated README.md with bilingual documentation")

    def create_system_docs(self) -> None:
        """Create comprehensive system documentation."""
        docs_dir = self.project_root / "docs"
        docs_dir.mkdir(exist_ok=True)
        
        # Create API documentation
        self._create_api_documentation(docs_dir)
        
        # Create deployment guide
        self._create_deployment_guide(docs_dir)
        
        # Create troubleshooting guide
        self._create_troubleshooting_guide(docs_dir)
        
        print(f"âœ… Created system documentation in {docs_dir}")

    def _create_api_documentation(self, docs_dir: Path) -> None:
        """Create API documentation."""
        api_content = """# IndexTTS2 API Documentation

## Core Classes

### IndexTTS2

Main inference class for text-to-speech generation.

```python
from indextts.infer_v2 import IndexTTS2

tts = IndexTTS2(
    device="cuda",  # or "cpu", "mps"
    fp16=True,      # Use FP16 for memory efficiency
    deepspeed=False # Enable DeepSpeed acceleration
)
```

#### Methods

##### `infer(text, prompt_audio, emotion=None, **kwargs)`

Generate speech from text with optional emotion control.

**Parameters:**
- `text` (str): Input text to synthesize
- `prompt_audio` (str): Path to reference audio file
- `emotion` (str, optional): Emotion name or vector
- `language` (str): "zh" for Chinese, "en" for English
- `speed` (float): Speech speed multiplier (default: 1.0)

**Returns:**
- `numpy.ndarray`: Generated audio waveform

**Example:**
```python
audio = tts.infer(
    text="Hello world!",
    prompt_audio="examples/voice_01.wav",
    emotion="happy",
    language="en",
    speed=1.2
)
```

## Emotion Control

### Available Emotions

- `happy`: Joyful, upbeat expression
- `angry`: Aggressive, intense expression  
- `sad`: Melancholic, sorrowful expression
- `afraid`: Fearful, anxious expression
- `disgusted`: Repulsed, distasteful expression
- `melancholic`: Deep sadness, contemplative
- `surprised`: Shocked, amazed expression
- `calm`: Peaceful, relaxed expression

### Emotion Vectors

You can also use emotion vectors for fine-grained control:

```python
import numpy as np

# Custom emotion vector (8 dimensions)
emotion_vector = np.array([0.8, 0.2, 0.1, 0.0, 0.0, 0.3, 0.1, 0.5])

audio = tts.infer(
    text="Custom emotion example",
    prompt_audio="examples/voice_01.wav",
    emotion=emotion_vector
)
```

## Configuration

### Model Configuration

Edit `checkpoints/config.yaml` to modify model behavior:

```yaml
model:
  gpt_path: "checkpoints/gpt.pth"
  s2mel_path: "checkpoints/s2mel.pth"
  bigvgan_path: "checkpoints/bigvgan.pth"
  
inference:
  sample_rate: 22050
  hop_length: 256
  win_length: 1024
```

### Performance Tuning

```python
# Memory-efficient inference
tts = IndexTTS2(fp16=True, device="cuda")

# Maximum quality (requires more VRAM)
tts = IndexTTS2(fp16=False, cuda_kernel=True)

# CPU inference
tts = IndexTTS2(device="cpu")
```
"""
        
        with open(docs_dir / "API.md", 'w', encoding='utf-8') as f:
            f.write(api_content)

    def _create_deployment_guide(self, docs_dir: Path) -> None:
        """Create deployment guide."""
        deployment_content = """# Deployment Guide

## Production Deployment

### Docker Deployment

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY . .

# Install UV
RUN pip install -U uv

# Install dependencies
RUN uv sync --all-extras

# Download models
RUN uv tool install "huggingface_hub[cli]"
RUN hf download IndexTeam/IndexTTS-2 --local-dir=checkpoints

EXPOSE 7860

CMD ["uv", "run", "webui.py", "--server-name", "0.0.0.0"]
```

### Cloud Deployment

#### AWS EC2
1. Launch GPU instance (g4dn.xlarge or larger)
2. Install CUDA drivers
3. Follow standard installation process
4. Configure security groups for port 7860

#### Google Cloud Platform
1. Create Compute Engine instance with GPU
2. Install NVIDIA drivers
3. Follow standard installation process

### Performance Considerations

- **Memory**: 8GB+ VRAM recommended for optimal performance
- **Storage**: 10GB+ for models and cache
- **CPU**: Multi-core recommended for preprocessing
- **Network**: High bandwidth for model downloads

### Scaling

For high-throughput applications:

```python
# Use multiple workers
import multiprocessing as mp

def worker_process():
    tts = IndexTTS2(device=f"cuda:{worker_id}")
    # Process requests...

# Start multiple workers
workers = []
for i in range(mp.cpu_count()):
    p = mp.Process(target=worker_process)
    workers.append(p)
    p.start()
```

### Monitoring

Monitor GPU usage and memory:

```bash
# GPU monitoring
nvidia-smi -l 1

# Memory usage
uv run tools/gpu_check.py
```
"""
        
        with open(docs_dir / "DEPLOYMENT.md", 'w', encoding='utf-8') as f:
            f.write(deployment_content)

    def _create_troubleshooting_guide(self, docs_dir: Path) -> None:
        """Create troubleshooting guide."""
        troubleshooting_content = """# Troubleshooting Guide

## Common Issues

### Installation Issues

#### UV Installation Failed
```bash
# Try alternative installation
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### CUDA Not Detected
```bash
# Check CUDA installation
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstall PyTorch with CUDA
uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Model Download Failed
```bash
# Use alternative download method
wget https://huggingface.co/IndexTeam/IndexTTS-2/resolve/main/gpt.pth -O checkpoints/gpt.pth
```

### Runtime Issues

#### Out of Memory (OOM)
- Enable FP16: `--fp16`
- Reduce batch size
- Use CPU inference: `--device cpu`
- Close other GPU applications

#### Slow Inference
- Enable CUDA kernels: `--cuda_kernel`
- Use FP16 precision: `--fp16`
- Check GPU utilization: `nvidia-smi`

#### Audio Quality Issues
- Check sample rate matches (22050 Hz)
- Verify prompt audio quality
- Try different emotion settings
- Ensure proper text preprocessing

### Web UI Issues

#### Port Already in Use
```bash
# Use different port
uv run webui.py --server-port 7861
```

#### Cannot Access Web UI
- Check firewall settings
- Use `--server-name 0.0.0.0` for external access
- Verify port forwarding

### Model Issues

#### Model Loading Failed
- Verify checkpoints directory structure
- Check file permissions
- Re-download corrupted models

#### Emotion Control Not Working
- Verify emotion name spelling
- Check emotion vector dimensions (must be 8)
- Ensure Qwen model is loaded

## Performance Optimization

### Memory Optimization
```python
# Optimize for low VRAM
tts = IndexTTS2(
    fp16=True,
    device="cuda",
    low_vram_mode=True
)
```

### Speed Optimization
```python
# Optimize for speed
tts = IndexTTS2(
    cuda_kernel=True,
    compile_model=True
)
```

## Getting Help

1. Check this troubleshooting guide
2. Search existing GitHub issues
3. Create new issue with:
   - System information
   - Error messages
   - Minimal reproduction code
   - Expected vs actual behavior

## Debug Information

Run diagnostic script:
```bash
uv run tools/gpu_check.py
```

This will output:
- System information
- GPU details
- Memory usage
- Model status
"""
        
        with open(docs_dir / "TROUBLESHOOTING.md", 'w', encoding='utf-8') as f:
            f.write(troubleshooting_content)

    def update_version_info(self) -> None:
        """Update version information in documentation."""
        # This method can be used to update version-specific information
        # in documentation files when versions change
        metadata = self.get_project_metadata()
        current_version = metadata['version']
        
        print(f"âœ… Version information updated to {current_version}")

    def generate_all_documentation(self) -> None:
        """Generate all documentation files."""
        print("ðŸš€ Generating bilingual documentation...")
        
        # Update main README
        self.update_readme()
        
        # Create system documentation
        self.create_system_docs()
        
        # Update version info
        self.update_version_info()
        
        print("âœ… All documentation generated successfully!")


if __name__ == "__main__":
    # Example usage
    generator = DocumentationGenerator()
    generator.generate_all_documentation()